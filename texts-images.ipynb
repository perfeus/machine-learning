{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuU5ZQtb4Qc-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXfSY8mo4QdD"
   },
   "source": [
    "---\n",
    "# Part 3. Extract feature from texts and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Jc_YQXt4QdF"
   },
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pH4pkQP4QdF",
    "outputId": "905d010f-e4dc-426f-a9b3-b2cb62c05128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n",
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "# load the dataset from sklearn\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits['images'].shape)\n",
    "print(digits['data'].shape)\n",
    "print(digits['target'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9SYNAM-4QdG",
    "outputId": "9ddbf417-3e6a-41d9-9aed-e1984426221f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images contain 8x8 black'n'white pictures of hadwritten digits\n",
    "digits['images'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqlhH7tF4QdH"
   },
   "source": [
    "#### 1. Explore the data\n",
    "\n",
    "Let's plot several pictures, split dataset on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbaZ9y7g4QdH"
   },
   "outputs": [],
   "source": [
    "# make a plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCkuzU7f4QdI"
   },
   "source": [
    "We will assume that each pixel is a feature. Note, that images were already rechaped into vectors for us in `digits['data']` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkivG_hl4QdJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# perform train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNezNu-g4QdK"
   },
   "source": [
    "#### 2. Train the model\n",
    "\n",
    "We can now train the model. This is a classification task with 10 classes. \n",
    "\n",
    "The model, that we will use is called `KNeighborsClassifier` (kNN). It does not have trainable parameters and works in the following manner:\n",
    "* Remember the training data\n",
    "* When new point arrives \n",
    "    * find `K` nearest points in the training dataset (e.g. by euclidean distance)\n",
    "    * return the most freaquent class among these `K`.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmoukJ2r4QdK"
   },
   "source": [
    "   Create and train the following pipeline:\n",
    "* Scale the input vectorized image \n",
    "* Classify by kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEqBaOgi4QdL"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# define pipeline\n",
    "\n",
    "# fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIH9wLQ64QdL"
   },
   "source": [
    "#### 3. Evaluate on test dataset\n",
    "\n",
    "We will use accuracy - proportion of correct predictions, to measure the quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsZohRww4QdM"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# predict on test set\n",
    "\n",
    "# compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywXnQUBu4QdM",
    "outputId": "c02e83c0-4377-44c2-d13d-fcd423f523cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9755555555555555\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sszKzt_4QdM"
   },
   "source": [
    "**Optional task**\n",
    "\n",
    "Implement accuracy youself usinf numpy only;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dX6f9u7G4QdN"
   },
   "outputs": [],
   "source": [
    "def my_accuracy(true, predicted):\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "# test that your function work the same as `accuracy_score` from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDEsqWLh4QdN"
   },
   "source": [
    "---\n",
    "## Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7Z_GqsT4QdN"
   },
   "source": [
    "### 1.1 Convert documents to vectors\n",
    "\n",
    "Note that out dataset now is a set of documents (or texts): $D = (d_1, \\dots, d_N)$\n",
    "\n",
    "We will assume that there is vocabulary of size $M$, which contains all possible words, from which our documents are composed. \n",
    "\n",
    "Of course, each documents has different number of words in it. \n",
    "\n",
    "Today, we will consider 2 options how to represent texts with the vectors (embeddings):\n",
    "1. Bag of words\n",
    "2. Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4mNRe_t4QdO"
   },
   "source": [
    "Let us firstly use the simplest example to undertand how both methods work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F981PirV4QdO"
   },
   "outputs": [],
   "source": [
    "d1 = \"This is my favourite movie\"\n",
    "d2 = \"Is this movie boring? Yes, it is!\"\n",
    "d3 = \"This is an exiting movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3Ba4fb74QdO",
    "outputId": "53cab14b-1097-40eb-87a9-17e0750476c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'my', 'favourite', 'movie'],\n",
       " ['is', 'this', 'movie', 'boring', 'yes', 'it', 'is'],\n",
       " ['this', 'is', 'an', 'exiting', 'movie']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "D = [re.sub('[.!?,]', '', d.lower()).split(' ') for d in [d1, d2, d3]]\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNOKR6Zi4QdP"
   },
   "source": [
    "The Vocabulary for such dataset would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vfml28yB4QdP",
    "outputId": "f70224a2-489f-41b1-9377-007e55889106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boring', 'movie', 'exiting', 'my', 'yes', 'is', 'an', 'favourite', 'it', 'this']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Et98iA24QdP"
   },
   "source": [
    "#### Option 1. Bag of words\n",
    "\n",
    "We can say, that text is charaterized by the vector of length $M$, which shows how many times each word from the vector is present in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d91Ihcsc4QdQ"
   },
   "source": [
    "Let us now calculate bag of words for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xja79zo4QdQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GW0YYb-O4QdQ",
    "outputId": "2adf5a6a-c73c-4160-fcef-93306903fe6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
       "       [1., 1., 0., 0., 1., 2., 0., 0., 1., 1.],\n",
       "       [0., 1., 1., 0., 0., 1., 1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmepH6I64QdQ"
   },
   "source": [
    "#### Option 2. Tf-idf\n",
    "\n",
    "**Term Frequency times Inverce Document Frequency**\n",
    "\n",
    "A method to describe each document in the dataset with a vector of the same length. Takes into account, how often the word appears in the whole dataset.\n",
    "\n",
    "\n",
    "\n",
    "**Term frequency (tf)** - number of times a term occurs in a given document\n",
    "$$\n",
    "tf(t, d) = \\frac{\\# t \\text{ in } d}{len(d)}\n",
    "$$\n",
    "\n",
    "\n",
    "**Inverce document frequency (idf)** - measures informativeness of a term\n",
    "\n",
    "$$\n",
    "idf(t) = \\log \\frac{N}{(\\# d \\text{ with } t)} , N - \\text{ number of documents}\n",
    "$$\n",
    "\n",
    "If the word occures almost in all the documents (e.g. article, popular verb), then $idf$ will be very low.\n",
    "\n",
    "---\n",
    "Now we can covert each document onti the vector of size $M$:\n",
    "$$\n",
    "d \\rightarrow \\left(tf(t_1, d)\\cdot idf(t_1),\\,\\, \\dots, \\,\\, tf(t_M, d) \\cdot idf(t_M)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "Let us calculate it for our simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0O5Sd3V4QdR",
    "outputId": "84a4a993-3e28-40f7-c733-31bd99f05e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'my', 'favourite', 'movie'], ['is', 'this', 'movie', 'boring', 'yes', 'it', 'is'], ['this', 'is', 'an', 'exiting', 'movie']] \n",
      "\n",
      "['boring', 'movie', 'exiting', 'my', 'yes', 'is', 'an', 'favourite', 'it', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(D, '\\n')\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcq9L9we4QdR"
   },
   "outputs": [],
   "source": [
    "# tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dn5FjwWX4QdR"
   },
   "outputs": [],
   "source": [
    "#idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlLOAI8l4QdR",
    "outputId": "ccc8db30-9daf-43db-ce42-3834b09051dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.22, 0.  , 0.  , 0.  , 0.22, 0.  , 0.  ],\n",
       "       [0.16, 0.  , 0.  , 0.  , 0.16, 0.  , 0.  , 0.  , 0.16, 0.  ],\n",
       "       [0.  , 0.  , 0.22, 0.  , 0.  , 0.  , 0.22, 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf*idf\n",
    "np.round(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx7JU9sD4QdR"
   },
   "source": [
    "---\n",
    "**Optional task**\n",
    "\n",
    "Below we will use `BoW` and `tf-idf` features to classify texts from the dataset with news articles. \n",
    "\n",
    "### BoW and Tf-Idf in Sklearn\n",
    "\n",
    "In practice, we can use `Trasfromers` from sklearn to get the same vector representation of texts as we implemented above. \n",
    "\n",
    "#### 1. Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1899,
     "status": "ok",
     "timestamp": 1663443597998,
     "user": {
      "displayName": "Маргарита Бурова",
      "userId": "08611594987469395240"
     },
     "user_tz": -180
    },
    "id": "A42p6sne4QdR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "categories = ['alt.atheism', 'sci.space']\n",
    "train = pd.read_csv('https://github.com/mbburova/MDS/raw/main/train_news.csv', index_col=0)\n",
    "X_train, y_train = train.news, train.target\n",
    "\n",
    "\n",
    "test = pd.read_csv('https://github.com/mbburova/MDS/raw/main/test_news.csv', index_col=0)\n",
    "X_test, y_test = test.news, test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpaSPdBP4QdS"
   },
   "source": [
    "#### 2. Explore the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9KYgGFp4QdS",
    "outputId": "9742029c-80fd-4919-debf-e5d9ab48896b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0\n",
      ": \n",
      ": >> Please enlighten me.  How is omnipotence contradictory?\n",
      ": \n",
      ": >By definition, all that can occur in the universe is governed by the rules\n",
      ": >of nature. Thus god cannot break them. Anything that god does must be allowed\n",
      ": >in the rules somewhere. Therefore, omnipotence CANNOT exist! It contradicts\n",
      ": >the rules of nature.\n",
      ": \n",
      ": Obviously, an omnipotent god can change the rules.\n",
      "\n",
      "When you say, \"By definition\", what exactly is being defined;\n",
      "certainly not omnipotence. You seem to be saying that the \"rules of\n",
      "nature\" are pre-existant somehow, that they not only define nature but\n",
      "actually cause it. If that's what you mean I'd like to hear your\n",
      "further thoughts on the question.\n",
      "-------\n",
      "\n",
      "label: 1\n",
      "In <19APR199320262420@kelvin.jpl.nasa.gov> baalke@kelvin.jpl.nasa.gov \n",
      "\n",
      "Sorry I think I missed a bit of info on this Transition Experiment. What is it?\n",
      "\n",
      "Will this mean a loss of data or will the Magellan transmit data later on ??\n",
      "\n",
      "BTW: When will NASA cut off the connection with Magellan?? Not that I am\n",
      "looking forward to that day but I am just curious. I believe it had something\n",
      "to do with the funding from the goverment (or rather _NO_ funding :-)\n",
      "\n",
      "ok that's it for now. See you guys around,\n",
      "Jurriaan.\n",
      " \n",
      "-------\n",
      "\n",
      "label: 1\n",
      "\n",
      "Henry, I made the assumption that he who gets there firstest with the mostest\n",
      "wins. \n",
      "\n",
      "Ohhh, you want to put in FINE PRINT which says \"Thou shall do wonderous R&D\n",
      "rather than use off-the-shelf hardware\"? Sorry, didn't see that in my copy.\n",
      "Most of the Pournellesque proposals run along the lines of <some dollar\n",
      "amount> reward for <some simple goal>.  \n",
      "\n",
      "You go ahead and do your development, I'll buy off the shelf at higher cost (or\n",
      "even Russian; but I also assume that there'd be some \"Buy US\" provos in there)\n",
      "and be camped out in the Moon while you are launching and assembling little\n",
      "itty-bitty payloads in LEO with your laser or gas gun.  And working out the\n",
      "bugs of assembly & integration in LEO. \n",
      "\n",
      "Oh, hey, could I get a couple of CanadARMs tuned for the lunar environment?  I\n",
      "wanna do some teleoperated prospecting while I'm up there...\n",
      "\n",
      "\n",
      "\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('label:',y_train[i])\n",
    "    print(X_train[i])\n",
    "    print('-------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c1nD2jP4QdS"
   },
   "source": [
    "#### 3.1 BoW \n",
    "\n",
    "Our pipeline:\n",
    "* BoW vectorizer\n",
    "* kNN classifier\n",
    "\n",
    "We will use accuracy to evaluate model on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43ND2XcG4QdS"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTi9h2AD4QdS"
   },
   "outputs": [],
   "source": [
    "# Define traning pipeline\n",
    "bow = CountVectorizer(min_df=0.1, stop_words='english')\n",
    "\n",
    "pipe = # your code here\n",
    "\n",
    "# Fit on train data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test data (compute accuracy)\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4LZD-KT4QdT"
   },
   "source": [
    "#### 3.2 Tf-Idf\n",
    "\n",
    "Let's repeat the same procedure, but for tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FurFH7_4QdT"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define traning pipeline\n",
    "tf_idf = TfidfVectorizer(sublinear_tf=True, min_df=0.1, stop_words='english')\n",
    "\n",
    "pipe = # your code here\n",
    "\n",
    "\n",
    "# Fit\n",
    "# your code here\n",
    "\n",
    "# Evaluate on test\n",
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
